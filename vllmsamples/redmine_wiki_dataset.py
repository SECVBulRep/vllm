"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è Q&A –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ Redmine Wiki (PostgreSQL)
–§–æ—Ä–º–∞—Ç: ShareGPT (–¥–ª—è Qwen + LLaMA-Factory)

–õ–æ–∫–∞–ª—å–Ω–∞—è LLM: openai/gpt-oss-20b –Ω–∞ kurchatov-mini:8000
–°—Ç—Ä–∞–Ω–∏—Ü—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –¶–ï–õ–ò–ö–û–ú.

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã:
  - –ó–∞–ø–æ–º–∏–Ω–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ progress.json
  - –ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –∑–∞–ø—É—Å–∫–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
  - –ù–æ–≤—ã–µ Q&A –¥–æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
  - --reset –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏ –Ω–∞—á–∞–ª–∞ —Å –Ω—É–ª—è

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
  pip install psycopg2-binary requests
  python redmine_wiki_dataset.py --output dataset.json
  python redmine_wiki_dataset.py --output dataset.json          # –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫ ‚Äî –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ
  python redmine_wiki_dataset.py --output dataset.json --reset  # –Ω–∞—á–∞—Ç—å —Å –Ω—É–ª—è
"""

import json
import argparse
import re
import textwrap
import time
import sys
import os
import hashlib
from pathlib import Path

# ============================================================
# 1. –ù–ê–°–¢–†–û–ô–ö–ò
# ============================================================
DB_CONFIG = {
    "host": "irinka.webs.ru",
    "port": 5432,
    "dbname": "wiki_production",
    "user": "bulat",
    "password": "1234567809",
}

LLM_CONFIG = {
    "url": "http://kurchatov-mini:8000/v1/chat/completions",
    "model": "openai/gpt-oss-20b",
    "temperature": 0.1,
    "max_tokens": 10000,
    "top_p": 1.0,
    "frequency_penalty": 0,
    "presence_penalty": 0,
}

SYSTEM_PROMPT = (
    "–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏. "
    "–û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Ç–æ—á–Ω–æ, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é. "
    "–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π, —Å–æ–æ–±—â–∏ –æ–± —ç—Ç–æ–º."
)

QA_PER_PAGE_MIN = 3
QA_PER_PAGE_MAX = 10
LLM_DELAY = 1.0
LLM_RETRIES = 2  # –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON

# ============================================================
# 2. –ü–†–û–ì–†–ï–°–° ‚Äî –ó–ê–ü–û–ú–ò–ù–ê–ù–ò–ï –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• –°–¢–†–ê–ù–ò–¶
# ============================================================
class ProgressTracker:
    """
    –•—Ä–∞–Ω–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö.
    –ö–ª—é—á = page_id (–∏–∑ –ë–î) + hash —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ.
    –ï—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑–º–µ–Ω–∏–ª–æ—Å—å ‚Äî –æ–Ω–∞ –±—É–¥–µ—Ç –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞.
    """

    def __init__(self, progress_file: str):
        self.progress_file = progress_file
        self.data = self._load()

    def _load(self) -> dict:
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError):
                return {"processed": {}}
        return {"processed": {}}

    def save(self):
        with open(self.progress_file, "w", encoding="utf-8") as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)

    def is_processed(self, page_key: str, content_hash: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å —Ç–∞–∫–∏–º –∂–µ —Å–æ–¥–µ—Ä–∂–∏–º—ã–º."""
        entry = self.data.get("processed", {}).get(page_key)
        if entry and entry.get("content_hash") == content_hash:
            return True
        return False

    def mark_processed(self, page_key: str, content_hash: str, qa_count: int):
        """–û—Ç–º–µ—á–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é."""
        if "processed" not in self.data:
            self.data["processed"] = {}
        self.data["processed"][page_key] = {
            "content_hash": content_hash,
            "qa_count": qa_count,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        }
        self.save()

    def reset(self):
        """–û—á–∏—â–∞–µ—Ç –≤–µ—Å—å –ø—Ä–æ–≥—Ä–µ—Å—Å."""
        self.data = {"processed": {}}
        self.save()

    @property
    def total_processed(self) -> int:
        return len(self.data.get("processed", {}))


def content_hash(text: str) -> str:
    """MD5-—Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    return hashlib.md5((text or "").encode("utf-8")).hexdigest()


def page_key(page: dict) -> str:
    """–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á —Å—Ç—Ä–∞–Ω–∏—Ü—ã: project_id/page_title."""
    return f"{page['project_id']}/{page['page_title']}"


# ============================================================
# 3. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –î–ê–ù–ù–´–• –ò–ó POSTGRESQL
# ============================================================
WIKI_QUERY = """
SELECT DISTINCT ON (wp.id)
    wp.id                           AS wp_id,
    p.name                          AS project_name,
    p.identifier                    AS project_id,
    wp.title                        AS page_title,
    wc.text                         AS page_content,
    wc.updated_on                   AS updated_on,
    COALESCE(u.firstname || ' ' || u.lastname, '') AS author,
    wp.parent_id                    AS parent_page_id,
    parent_wp.title                 AS parent_page_title
FROM wiki_contents wc
JOIN wiki_pages wp        ON wc.page_id = wp.id
JOIN wikis w              ON wp.wiki_id = w.id
JOIN projects p           ON w.project_id = p.id
LEFT JOIN users u         ON wc.author_id = u.id
LEFT JOIN wiki_pages parent_wp ON wp.parent_id = parent_wp.id
WHERE wp.deleted_at IS NULL
  AND wc.text IS NOT NULL
  AND LENGTH(TRIM(wc.text)) > 50
ORDER BY wp.id, wc.version DESC;
"""


def fetch_wiki_pages(db_config: dict) -> list[dict]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç wiki-—Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ Redmine PostgreSQL."""
    import psycopg2
    import psycopg2.extras

    conn = psycopg2.connect(**db_config)
    try:
        with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
            cur.execute(WIKI_QUERY)
            rows = cur.fetchall()
            return [dict(r) for r in rows]
    finally:
        conn.close()


# ============================================================
# 4. –û–ß–ò–°–¢–ö–ê REDMINE WIKI-–†–ê–ó–ú–ï–¢–ö–ò
# ============================================================
def clean_wiki_text(text: str) -> str:
    """–£–±–∏—Ä–∞–µ—Ç Redmine/Textile —Ä–∞–∑–º–µ—Ç–∫—É, –æ—Å—Ç–∞–≤–ª—è—è —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç."""
    if not text:
        return ""

    text = re.sub(r'\{\{.*?\}\}', '', text)
    text = re.sub(r'h[1-6]\.\s*', '', text)
    text = re.sub(r'\*([^*]+)\*', r'\1', text)
    text = re.sub(r'_([^_]+)_', r'\1', text)
    text = re.sub(r'\[\[([^|\]]+)\|([^\]]+)\]\]', r'\2', text)
    text = re.sub(r'\[\[([^\]]+)\]\]', r'\1', text)
    text = re.sub(r'"([^"]+)":\S+', r'\1', text)
    text = re.sub(r'</?(?:pre|code)[^>]*>', '', text)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'^[*#]+\s*', '- ', text, flags=re.MULTILINE)
    text = re.sub(r'\|_\.', '', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = text.strip()

    return text


# ============================================================
# 5. –ì–ï–ù–ï–†–ê–¶–ò–Ø Q&A –ß–ï–†–ï–ó –õ–û–ö–ê–õ–¨–ù–£–Æ LLM
# ============================================================
def call_llm(prompt: str) -> str:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π LLM."""
    import requests

    payload = {
        "model": LLM_CONFIG["model"],
        "messages": [
            {
                "role": "system",
                "content": "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –æ–±—É—á–∞—é—â–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ì–µ–Ω–µ—Ä–∏—Ä—É–π —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON."
            },
            {"role": "user", "content": prompt},
        ],
        "temperature": LLM_CONFIG["temperature"],
        "max_tokens": LLM_CONFIG["max_tokens"],
        "top_p": LLM_CONFIG["top_p"],
        "frequency_penalty": LLM_CONFIG["frequency_penalty"],
        "presence_penalty": LLM_CONFIG["presence_penalty"],
    }

    resp = requests.post(
        LLM_CONFIG["url"],
        headers={"Content-Type": "application/json"},
        json=payload,
        timeout=180,
    )
    resp.raise_for_status()
    return resp.json()["choices"][0]["message"]["content"]


def parse_json_from_llm(text: str) -> list[dict]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON-–º–∞—Å—Å–∏–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM."""
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    text = text.strip()

    match = re.search(r'\[.*\]', text, re.DOTALL)
    if match:
        text = match.group(0)

    return json.loads(text)


def generate_qa_for_page(page: dict) -> list[dict]:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ LLM –∏ –ø–æ–ª—É—á–∞–µ—Ç Q&A –ø–∞—Ä—ã."""
    title = page["page_title"].replace("_", " ")
    project = page["project_name"]
    raw_text = page["page_content"] or ""
    clean_text = clean_wiki_text(raw_text)

    if len(clean_text) < 30:
        return []

    text_len = len(clean_text)
    if text_len < 500:
        num_qa = QA_PER_PAGE_MIN
    elif text_len < 2000:
        num_qa = 5
    else:
        num_qa = QA_PER_PAGE_MAX

    prompt = textwrap.dedent(f"""\
        –¢—ã —Å–æ–∑–¥–∞—ë—à—å –æ–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.

        –ù–∏–∂–µ ‚Äî –ø–æ–ª–Ω–∞—è —Å—Ç–∞—Ç—å—è –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π wiki.
        –ü—Ä–æ–µ–∫—Ç: {project}
        –°—Ç—Ä–∞–Ω–∏—Ü–∞: {title}

        === –¢–ï–ö–°–¢ –°–¢–ê–¢–¨–ò ===
        {clean_text}
        === –ö–û–ù–ï–¶ –°–¢–ê–¢–¨–ò ===

        –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π —Å—Ç–∞—Ç—å–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Ä–æ–≤–Ω–æ {num_qa} –ø–∞—Ä "–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç".

        –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
        1. –í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏: –æ–±—â–∏–µ, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ.
        2. –í–æ–ø—Ä–æ—Å—ã ‚Äî —Ç–∞–∫–∏–µ, –∫–∞–∫–∏–µ —Ä–µ–∞–ª—å–Ω–æ –∑–∞–¥–∞–ª –±—ã —Å–æ—Ç—Ä—É–¥–Ω–∏–∫ –∫–æ–º–ø–∞–Ω–∏–∏.
        3. –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–ª–Ω—ã–º–∏ –∏ –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –¢–û–õ–¨–ö–û –Ω–∞ —Ç–µ–∫—Å—Ç–µ —Å—Ç–∞—Ç—å–∏.
        4. –û—Ç–≤–µ—Ç—ã ‚Äî —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–µ, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ, –Ω–µ –º–µ–Ω–µ–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        5. –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ.

        –í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –º–∞—Å—Å–∏–≤. –ù–∏–∫–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ.
        –£–±–µ–¥–∏—Å—å —á—Ç–æ –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–∫—Ä—ã—Ç—ã –∫–∞–≤—ã—á–∫–∞–º–∏.
        [
          {{"question": "...", "answer": "..."}},
          {{"question": "...", "answer": "..."}}
        ]
    """)

    # –ü–æ–ø—ã—Ç–∫–∏ —Å —Ä–µ—Ç—Ä–∞—è–º–∏
    for attempt in range(LLM_RETRIES + 1):
        try:
            response = call_llm(prompt)
            qa_pairs = parse_json_from_llm(response)

            results = []
            for qa in qa_pairs:
                q = qa.get("question", "").strip()
                a = qa.get("answer", "").strip()
                if q and a and len(a) > 20:
                    results.append(make_sharegpt_entry(
                        question=q,
                        answer=a,
                        project=project,
                        page_title=title,
                    ))
            if results:
                return results

        except json.JSONDecodeError as e:
            if attempt < LLM_RETRIES:
                print(f"\n    ‚ö† JSON –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{LLM_RETRIES+1}), –ø–æ–≤—Ç–æ—Ä...", end="")
                time.sleep(LLM_DELAY)
            else:
                print(f"\n    ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –¥–ª—è '{title}' –ø–æ—Å–ª–µ {LLM_RETRIES+1} –ø–æ–ø—ã—Ç–æ–∫")
                return fallback_template(page)
        except Exception as e:
            print(f"\n    ‚ö† –û—à–∏–±–∫–∞ LLM –¥–ª—è '{title}': {e}")
            return fallback_template(page)

    return fallback_template(page)


# ============================================================
# 6. –§–û–õ–õ–ë–≠–ö ‚Äî –®–ê–ë–õ–û–ù–ù–´–ï –í–û–ü–†–û–°–´
# ============================================================
FALLBACK_TEMPLATES = [
    "–ß—Ç–æ —Ç–∞–∫–æ–µ {title}?",
    "–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ {title}.",
    "–ö–∞–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –µ—Å—Ç—å –ø–æ —Ç–µ–º–µ ¬´{title}¬ª?",
]


def fallback_template(page: dict) -> list[dict]:
    title = page["page_title"].replace("_", " ")
    project = page["project_name"]
    clean_text = clean_wiki_text(page["page_content"] or "")

    if len(clean_text) < 30:
        return []

    results = []
    for tmpl in FALLBACK_TEMPLATES:
        results.append(make_sharegpt_entry(
            question=tmpl.format(title=title),
            answer=clean_text,
            project=project,
            page_title=title,
        ))
    return results


# ============================================================
# 7. –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï ShareGPT –ó–ê–ü–ò–°–ò
# ============================================================
def make_sharegpt_entry(question: str, answer: str, project: str, page_title: str) -> dict:
    return {
        "conversations": [
            {"from": "system", "value": SYSTEM_PROMPT},
            {"from": "human", "value": question},
            {"from": "gpt", "value": answer},
        ],
        "metadata": {
            "project": project,
            "page": page_title,
        }
    }


# ============================================================
# 8. –ó–ê–ì–†–£–ó–ö–ê –°–£–©–ï–°–¢–í–£–Æ–©–ï–ì–û –î–ê–¢–ê–°–ï–¢–ê
# ============================================================
def load_existing_dataset(output_path: str) -> list[dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–æ–ø–∏—Å—ã–≤–∞–Ω–∏—è."""
    if os.path.exists(output_path):
        try:
            with open(output_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç: {len(data)} –∑–∞–ø–∏—Å–µ–π")
            return data
        except (json.JSONDecodeError, IOError) as e:
            print(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {output_path}: {e}")
            return []
    return []


# ============================================================
# 9. –û–°–ù–û–í–ù–û–ô –ü–ê–ô–ü–õ–ê–ô–ù
# ============================================================
def build_dataset(skip_llm: bool, output_path: str, progress_file: str) -> list[dict]:
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    tracker = ProgressTracker(progress_file)
    print(f"üì¶ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL (irinka.webs.ru / wiki_production)...")

    pages = fetch_wiki_pages(DB_CONFIG)
    print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ wiki-—Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages)}")

    if not pages:
        print("‚ùå –°—Ç—Ä–∞–Ω–∏—Ü –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return []

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
    pages_to_process = []
    pages_skipped = 0
    for page in pages:
        pk = page_key(page)
        ch = content_hash(page["page_content"] or "")
        if tracker.is_processed(pk, ch):
            pages_skipped += 1
        else:
            pages_to_process.append(page)

    print(f"   –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–∞–Ω–µ–µ: {pages_skipped}")
    print(f"   –ù–æ–≤—ã—Ö / –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö:   {len(pages_to_process)}")

    if not pages_to_process:
        print("‚úÖ –í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã. –ù–µ—á–µ–≥–æ –¥–µ–ª–∞—Ç—å.")
        print("   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --reset —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ.")
        return load_existing_dataset(output_path)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º LLM
    if not skip_llm:
        print(f"ü§ñ –ü—Ä–æ–≤–µ—Ä–∫–∞ LLM ({LLM_CONFIG['url']})...")
        try:
            test = call_llm("–û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: —Ä–∞–±–æ—Ç–∞–µ—Ç?")
            print(f"   ‚úÖ LLM –¥–æ—Å—Ç—É–ø–Ω–∞: {test[:50]}...")
        except Exception as e:
            print(f"   ‚ùå LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
            print("   –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ —à–∞–±–ª–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º.")
            skip_llm = True

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–æ–ø–∏—Å—ã–≤–∞–Ω–∏—è
    dataset = load_existing_dataset(output_path)
    new_count = 0
    errors = 0

    for i, page in enumerate(pages_to_process):
        title = page["page_title"]
        text = page["page_content"] or ""
        pk = page_key(page)
        ch = content_hash(text)

        print(f"  [{i+1}/{len(pages_to_process)}] {page['project_name']} / {title} ({len(text)} —Å–∏–º–≤.) ", end="")

        if skip_llm:
            examples = fallback_template(page)
        else:
            examples = generate_qa_for_page(page)
            if not examples:
                errors += 1
            time.sleep(LLM_DELAY)

        dataset.extend(examples)
        new_count += len(examples)

        # –û—Ç–º–µ—á–∞–µ–º –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é
        tracker.mark_processed(pk, ch, len(examples))

        print(f"‚Üí {len(examples)} Q&A ‚úì")

    print(f"\n{'='*50}")
    print(f"‚úÖ –ù–æ–≤—ã—Ö Q&A –ø—Ä–∏–º–µ—Ä–æ–≤:  {new_count}")
    print(f"‚úÖ –í—Å–µ–≥–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:    {len(dataset)}")
    if errors:
        print(f"‚ö†  –û—à–∏–±–æ–∫ LLM (—Ñ–æ–ª–ª–±—ç–∫): {errors}")

    return dataset


def main():
    parser = argparse.ArgumentParser(description="Redmine Wiki ‚Üí Q&A Dataset –¥–ª—è Qwen LoRA")
    parser.add_argument("--output", default="dataset.json",
                        help="–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É")
    parser.add_argument("--progress", default="progress.json",
                        help="–§–∞–π–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: progress.json)")
    parser.add_argument("--llm-url", default=None,
                        help="URL LLM API (–Ω–∞–ø—Ä–∏–º–µ—Ä http://172.16.29.232:8000/v1/chat/completions)")
    parser.add_argument("--skip-llm", action="store_true",
                        help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å LLM, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —à–∞–±–ª–æ–Ω—ã")
    parser.add_argument("--reset", action="store_true",
                        help="–û—á–∏—Å—Ç–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –∏ –Ω–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ")
    parser.add_argument("--delay", type=float, default=1.0,
                        help="–ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ LLM, —Å–µ–∫")
    parser.add_argument("--qa-min", type=int, default=3,
                        help="–ú–∏–Ω–∏–º—É–º Q&A –ø–∞—Ä –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É")
    parser.add_argument("--qa-max", type=int, default=7,
                        help="–ú–∞–∫—Å–∏–º—É–º Q&A –ø–∞—Ä –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É")
    parser.add_argument("--retries", type=int, default=2,
                        help="–ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ JSON")
    args = parser.parse_args()

    if args.llm_url:
        LLM_CONFIG["url"] = args.llm_url
        print(f"üîó LLM URL: {args.llm_url}")

    global LLM_DELAY, QA_PER_PAGE_MIN, QA_PER_PAGE_MAX, LLM_RETRIES
    LLM_DELAY = args.delay
    QA_PER_PAGE_MIN = args.qa_min
    QA_PER_PAGE_MAX = args.qa_max
    LLM_RETRIES = args.retries

    # –°–±—Ä–æ—Å –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    if args.reset:
        tracker = ProgressTracker(args.progress)
        tracker.reset()
        if os.path.exists(args.output):
            os.remove(args.output)
        print("üîÑ –ü—Ä–æ–≥—Ä–µ—Å—Å –∏ –¥–∞—Ç–∞—Å–µ—Ç –æ—á–∏—â–µ–Ω—ã.")

    dataset = build_dataset(
        skip_llm=args.skip_llm,
        output_path=args.output,
        progress_file=args.progress,
    )

    if not dataset:
        print("‚ùå –î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç.")
        sys.exit(1)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    with open(args.output, "w", encoding="utf-8") as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
    print(f"\nüíæ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {args.output}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    projects = set(e["metadata"]["project"] for e in dataset)
    pages_set = set(e["metadata"]["page"] for e in dataset)
    avg_q = sum(len(e["conversations"][1]["value"]) for e in dataset) / len(dataset)
    avg_a = sum(len(e["conversations"][2]["value"]) for e in dataset) / len(dataset)

    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –ü—Ä–æ–µ–∫—Ç–æ–≤:              {len(projects)}")
    print(f"   Wiki-—Å—Ç—Ä–∞–Ω–∏—Ü:          {len(pages_set)}")
    print(f"   Q&A –ø–∞—Ä:               {len(dataset)}")
    print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞: {avg_q:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞:  {avg_a:.0f} —Å–∏–º–≤–æ–ª–æ–≤")

    # –ü—Ä–∏–º–µ—Ä
    if dataset:
        print(f"\nüìù –ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏:")
        example = dataset[0].copy()
        ans = example["conversations"][2]["value"]
        if len(ans) > 300:
            example["conversations"][2]["value"] = ans[:300] + "..."
        print(json.dumps(example, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()