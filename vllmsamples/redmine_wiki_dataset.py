"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è Q&A –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ Redmine Wiki (PostgreSQL)
–§–æ—Ä–º–∞—Ç: ShareGPT (–¥–ª—è Qwen + LLaMA-Factory)

–õ–æ–∫–∞–ª—å–Ω–∞—è LLM: openai/gpt-oss-20b –Ω–∞ kurchatov-mini:8000
–°—Ç—Ä–∞–Ω–∏—Ü—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –¶–ï–õ–ò–ö–û–ú (–±–µ–∑ —Ä–∞–∑–±–∏–≤–∫–∏ –Ω–∞ —á–∞–Ω–∫–∏).

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
  pip install psycopg2-binary requests
  python redmine_wiki_dataset.py --output dataset.json
"""

import json
import argparse
import re
import textwrap
import time
import sys

# ============================================================
# 1. –ù–ê–°–¢–†–û–ô–ö–ò
# ============================================================
DB_CONFIG = {
    "host": "irinka.webs.ru",
    "port": 5432,
    "dbname": "wiki_production",
    "user": "bulat",
    "password": "1234567809",
}

LLM_CONFIG = {
    "url": "http://kurchatov-mini:8000/v1/chat/completions",
    "model": "openai/gpt-oss-20b",
    "temperature": 0.1,
    "max_tokens": 2000,
    "top_p": 1.0,
    "frequency_penalty": 0,
    "presence_penalty": 0,
}

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ Q&A –±–æ—Ç–∞
SYSTEM_PROMPT = (
    "–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏. "
    "–û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Ç–æ—á–Ω–æ, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é. "
    "–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π, —Å–æ–æ–±—â–∏ –æ–± —ç—Ç–æ–º."
)

# –°–∫–æ–ª—å–∫–æ Q&A –ø–∞—Ä –ø—Ä–æ—Å–∏—Ç—å —É LLM –Ω–∞ –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É
QA_PER_PAGE_MIN = 3
QA_PER_PAGE_MAX = 7

# –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ LLM (—Å–µ–∫—É–Ω–¥—ã)
LLM_DELAY = 1.0

# ============================================================
# 2. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –î–ê–ù–ù–´–• –ò–ó POSTGRESQL
# ============================================================
WIKI_QUERY = """
SELECT
    p.name                          AS project_name,
    p.identifier                    AS project_id,
    wp.title                        AS page_title,
    wc.text                         AS page_content,
    wc.updated_on                   AS updated_on,
    COALESCE(u.firstname || ' ' || u.lastname, '') AS author,
    wp.parent_id                    AS parent_page_id,
    parent_wp.title                 AS parent_page_title
FROM wiki_contents wc
JOIN wiki_pages wp        ON wc.page_id = wp.id
JOIN wikis w              ON wp.wiki_id = w.id
JOIN projects p           ON w.project_id = p.id
LEFT JOIN users u         ON wc.author_id = u.id
LEFT JOIN wiki_pages parent_wp ON wp.parent_id = parent_wp.id
WHERE wp.deleted_at IS NULL
  AND wc.text IS NOT NULL
  AND LENGTH(TRIM(wc.text)) > 50
ORDER BY p.name, wp.title LIMIT 5;
"""


def fetch_wiki_pages(db_config: dict) -> list[dict]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç wiki-—Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ Redmine PostgreSQL."""
    import psycopg2
    import psycopg2.extras

    conn = psycopg2.connect(**db_config)
    try:
        with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
            cur.execute(WIKI_QUERY)
            rows = cur.fetchall()
            return [dict(r) for r in rows]
    finally:
        conn.close()


# ============================================================
# 3. –û–ß–ò–°–¢–ö–ê REDMINE WIKI-–†–ê–ó–ú–ï–¢–ö–ò
# ============================================================
def clean_wiki_text(text: str) -> str:
    """–£–±–∏—Ä–∞–µ—Ç Redmine/Textile —Ä–∞–∑–º–µ—Ç–∫—É, –æ—Å—Ç–∞–≤–ª—è—è —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç."""
    if not text:
        return ""

    # –ú–∞–∫—Ä–æ—Å—ã {{...}}
    text = re.sub(r'\{\{.*?\}\}', '', text)

    # –ó–∞–≥–æ–ª–æ–≤–∫–∏ h1. h2. h3.
    text = re.sub(r'h[1-6]\.\s*', '', text)

    # –ñ–∏—Ä–Ω—ã–π *text* –∏ _–∫—É—Ä—Å–∏–≤_
    text = re.sub(r'\*([^*]+)\*', r'\1', text)
    text = re.sub(r'_([^_]+)_', r'\1', text)

    # –°—Å—ã–ª–∫–∏ [[Page|text]] –∏ [[Page]]
    text = re.sub(r'\[\[([^|\]]+)\|([^\]]+)\]\]', r'\2', text)
    text = re.sub(r'\[\[([^\]]+)\]\]', r'\1', text)

    # –í–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏ "text":url
    text = re.sub(r'"([^"]+)":\S+', r'\1', text)

    # –ö–æ–¥ <pre>, <code>
    text = re.sub(r'</?(?:pre|code)[^>]*>', '', text)

    # HTML-—Ç–µ–≥–∏
    text = re.sub(r'<[^>]+>', '', text)

    # –°–ø–∏—Å–∫–∏: * –∏–ª–∏ # –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏
    text = re.sub(r'^[*#]+\s*', '- ', text, flags=re.MULTILINE)

    # –¢–∞–±–ª–∏—Ü—ã |_. (–∑–∞–≥–æ–ª–æ–≤–∫–∏)
    text = re.sub(r'\|_\.', '', text)

    # –õ–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = text.strip()

    return text


# ============================================================
# 4. –ì–ï–ù–ï–†–ê–¶–ò–Ø Q&A –ß–ï–†–ï–ó –õ–û–ö–ê–õ–¨–ù–£–Æ LLM
# ============================================================
def call_llm(prompt: str) -> str:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π LLM –Ω–∞ kurchatov-mini."""
    import requests

    payload = {
        "model": LLM_CONFIG["model"],
        "messages": [
            {
                "role": "system",
                "content": "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –æ–±—É—á–∞—é—â–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ì–µ–Ω–µ—Ä–∏—Ä—É–π —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON."
            },
            {"role": "user", "content": prompt},
        ],
        "temperature": LLM_CONFIG["temperature"],
        "max_tokens": LLM_CONFIG["max_tokens"],
        "top_p": LLM_CONFIG["top_p"],
        "frequency_penalty": LLM_CONFIG["frequency_penalty"],
        "presence_penalty": LLM_CONFIG["presence_penalty"],
    }

    resp = requests.post(
        LLM_CONFIG["url"],
        headers={"Content-Type": "application/json"},
        json=payload,
        timeout=120,
    )
    resp.raise_for_status()
    return resp.json()["choices"][0]["message"]["content"]


def parse_json_from_llm(text: str) -> list[dict]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON-–º–∞—Å—Å–∏–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM (–¥–∞–∂–µ –µ—Å–ª–∏ –æ–±—ë—Ä–Ω—É—Ç –≤ markdown)."""
    # –£–±–∏—Ä–∞–µ–º ```json ... ```
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    text = text.strip()

    # –ò—â–µ–º –º–∞—Å—Å–∏–≤ [ ... ]
    match = re.search(r'\[.*\]', text, re.DOTALL)
    if match:
        text = match.group(0)

    return json.loads(text)


def generate_qa_for_page(page: dict) -> list[dict]:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ LLM –∏ –ø–æ–ª—É—á–∞–µ—Ç Q&A –ø–∞—Ä—ã.
    """
    title = page["page_title"].replace("_", " ")
    project = page["project_name"]
    raw_text = page["page_content"] or ""
    clean_text = clean_wiki_text(raw_text)

    if len(clean_text) < 30:
        return []

    # –ö–æ–ª-–≤–æ –ø–∞—Ä –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
    text_len = len(clean_text)
    if text_len < 500:
        num_qa = QA_PER_PAGE_MIN
    elif text_len < 2000:
        num_qa = 5
    else:
        num_qa = QA_PER_PAGE_MAX

    prompt = textwrap.dedent(f"""\
        –¢—ã —Å–æ–∑–¥–∞—ë—à—å –æ–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.

        –ù–∏–∂–µ ‚Äî –ø–æ–ª–Ω–∞—è —Å—Ç–∞—Ç—å—è –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π wiki.
        –ü—Ä–æ–µ–∫—Ç: {project}
        –°—Ç—Ä–∞–Ω–∏—Ü–∞: {title}

        === –¢–ï–ö–°–¢ –°–¢–ê–¢–¨–ò ===
        {clean_text}
        === –ö–û–ù–ï–¶ –°–¢–ê–¢–¨–ò ===

        –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π —Å—Ç–∞—Ç—å–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Ä–æ–≤–Ω–æ {num_qa} –ø–∞—Ä "–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç".

        –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
        1. –í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏: –æ–±—â–∏–µ, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ.
           –ü—Ä–∏–º–µ—Ä—ã —Ç–∏–ø–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤:
           - "–ß—Ç–æ —Ç–∞–∫–æ–µ ...?"
           - "–ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å ...?"
           - "–ö–∞–∫–∏–µ —à–∞–≥–∏ –Ω—É–∂–Ω—ã –¥–ª—è ...?"
           - "–ö—Ç–æ –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ ...?"
           - "–ì–¥–µ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ ...?"
        2. –í–æ–ø—Ä–æ—Å—ã ‚Äî —Ç–∞–∫–∏–µ, –∫–∞–∫–∏–µ —Ä–µ–∞–ª—å–Ω–æ –∑–∞–¥–∞–ª –±—ã —Å–æ—Ç—Ä—É–¥–Ω–∏–∫ –∫–æ–º–ø–∞–Ω–∏–∏.
        3. –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–ª–Ω—ã–º–∏ –∏ –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –¢–û–õ–¨–ö–û –Ω–∞ —Ç–µ–∫—Å—Ç–µ —Å—Ç–∞—Ç—å–∏.
        4. –û—Ç–≤–µ—Ç—ã ‚Äî —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–µ, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ, –Ω–µ –º–µ–Ω–µ–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        5. –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ.

        –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –º–∞—Å—Å–∏–≤, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤:
        [
          {{"question": "...", "answer": "..."}},
          {{"question": "...", "answer": "..."}}
        ]
    """)

    try:
        response = call_llm(prompt)
        qa_pairs = parse_json_from_llm(response)

        results = []
        for qa in qa_pairs:
            q = qa.get("question", "").strip()
            a = qa.get("answer", "").strip()
            if q and a and len(a) > 20:
                results.append(make_sharegpt_entry(
                    question=q,
                    answer=a,
                    project=project,
                    page_title=title,
                ))
        return results

    except json.JSONDecodeError as e:
        print(f"\n    ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –¥–ª—è '{title}': {e}")
        return fallback_template(page)
    except Exception as e:
        print(f"\n    ‚ö† –û—à–∏–±–∫–∞ LLM –¥–ª—è '{title}': {e}")
        return fallback_template(page)


# ============================================================
# 5. –§–û–õ–õ–ë–≠–ö ‚Äî –®–ê–ë–õ–û–ù–ù–´–ï –í–û–ü–†–û–°–´ (–µ—Å–ª–∏ LLM –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª–∞)
# ============================================================
FALLBACK_TEMPLATES = [
    "–ß—Ç–æ —Ç–∞–∫–æ–µ {title}?",
    "–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ {title}.",
    "–ö–∞–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –µ—Å—Ç—å –ø–æ —Ç–µ–º–µ ¬´{title}¬ª?",
]


def fallback_template(page: dict) -> list[dict]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ —à–∞–±–ª–æ–Ω–∞–º, –µ—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."""
    title = page["page_title"].replace("_", " ")
    project = page["project_name"]
    clean_text = clean_wiki_text(page["page_content"] or "")

    if len(clean_text) < 30:
        return []

    results = []
    for tmpl in FALLBACK_TEMPLATES:
        results.append(make_sharegpt_entry(
            question=tmpl.format(title=title),
            answer=clean_text,
            project=project,
            page_title=title,
        ))
    return results


# ============================================================
# 6. –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï ShareGPT –ó–ê–ü–ò–°–ò
# ============================================================
def make_sharegpt_entry(question: str, answer: str, project: str, page_title: str) -> dict:
    return {
        "conversations": [
            {"from": "system", "value": SYSTEM_PROMPT},
            {"from": "human", "value": question},
            {"from": "gpt", "value": answer},
        ],
        "metadata": {
            "project": project,
            "page": page_title,
        }
    }


# ============================================================
# 7. –û–°–ù–û–í–ù–û–ô –ü–ê–ô–ü–õ–ê–ô–ù
# ============================================================
def build_dataset(skip_llm: bool = False) -> list[dict]:
    print("üì¶ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL (irinka.webs.ru / wiki_production)...")
    pages = fetch_wiki_pages(DB_CONFIG)
    print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ wiki-—Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages)}")

    if not pages:
        print("‚ùå –°—Ç—Ä–∞–Ω–∏—Ü –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∏ –¥–∞–Ω–Ω—ã–µ –≤ –ë–î.")
        return []

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LLM
    if not skip_llm:
        print(f"ü§ñ –ü—Ä–æ–≤–µ—Ä–∫–∞ LLM (kurchatov-mini:8000)...")
        try:
            test = call_llm("–û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: —Ä–∞–±–æ—Ç–∞–µ—Ç?")
            print(f"   ‚úÖ LLM –¥–æ—Å—Ç—É–ø–Ω–∞: {test[:50]}...")
        except Exception as e:
            print(f"   ‚ùå LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
            print("   –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ —à–∞–±–ª–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º.")
            skip_llm = True

    dataset = []
    errors = 0

    for i, page in enumerate(pages):
        title = page["page_title"]
        text = page["page_content"] or ""
        print(f"  [{i+1}/{len(pages)}] {page['project_name']} / {title} ({len(text)} —Å–∏–º–≤.) ", end="")

        if skip_llm:
            examples = fallback_template(page)
        else:
            examples = generate_qa_for_page(page)
            if not examples:
                errors += 1
            time.sleep(LLM_DELAY)

        dataset.extend(examples)
        print(f"‚Üí {len(examples)} Q&A")

    print(f"\n{'='*50}")
    print(f"‚úÖ –í—Å–µ–≥–æ Q&A –ø—Ä–∏–º–µ—Ä–æ–≤: {len(dataset)}")
    if errors:
        print(f"‚ö†  –û—à–∏–±–æ–∫ LLM (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ñ–æ–ª–ª–±—ç–∫): {errors}")

    return dataset


def main():
    parser = argparse.ArgumentParser(description="Redmine Wiki ‚Üí Q&A Dataset –¥–ª—è Qwen LoRA")
    parser.add_argument("--output", default="dataset.json",
                        help="–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: dataset.json)")
    parser.add_argument("--skip-llm", action="store_true",
                        help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å LLM, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —à–∞–±–ª–æ–Ω—ã")
    parser.add_argument("--delay", type=float, default=1.0,
                        help="–ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ LLM, —Å–µ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1.0)")
    parser.add_argument("--qa-min", type=int, default=3,
                        help="–ú–∏–Ω–∏–º—É–º Q&A –ø–∞—Ä –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 3)")
    parser.add_argument("--qa-max", type=int, default=7,
                        help="–ú–∞–∫—Å–∏–º—É–º Q&A –ø–∞—Ä –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 7)")
    args = parser.parse_args()

    global LLM_DELAY, QA_PER_PAGE_MIN, QA_PER_PAGE_MAX
    LLM_DELAY = args.delay
    QA_PER_PAGE_MIN = args.qa_min
    QA_PER_PAGE_MAX = args.qa_max

    dataset = build_dataset(skip_llm=args.skip_llm)

    if not dataset:
        print("‚ùå –î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç.")
        sys.exit(1)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    with open(args.output, "w", encoding="utf-8") as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
    print(f"\nüíæ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {args.output}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    projects = set(e["metadata"]["project"] for e in dataset)
    pages_set = set(e["metadata"]["page"] for e in dataset)
    avg_q = sum(len(e["conversations"][1]["value"]) for e in dataset) / len(dataset)
    avg_a = sum(len(e["conversations"][2]["value"]) for e in dataset) / len(dataset)

    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –ü—Ä–æ–µ–∫—Ç–æ–≤:              {len(projects)}")
    print(f"   Wiki-—Å—Ç—Ä–∞–Ω–∏—Ü:          {len(pages_set)}")
    print(f"   Q&A –ø–∞—Ä:               {len(dataset)}")
    print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞: {avg_q:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞:  {avg_a:.0f} —Å–∏–º–≤–æ–ª–æ–≤")

    # –ü—Ä–∏–º–µ—Ä
    if dataset:
        print(f"\nüìù –ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–∏:")
        example = dataset[0].copy()
        # –û–±—Ä–µ–∑–∞–µ–º –æ—Ç–≤–µ—Ç –¥–ª—è –≤—ã–≤–æ–¥–∞
        ans = example["conversations"][2]["value"]
        if len(ans) > 300:
            example["conversations"][2]["value"] = ans[:300] + "..."
        print(json.dumps(example, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()