# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
"""
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å LoRA –∏ –±–µ–∑ LoRA
"""

import gc

import torch
from huggingface_hub import snapshot_download

from vllm import EngineArgs, LLMEngine, RequestOutput, SamplingParams
from vllm.lora.request import LoRARequest


def create_test_prompts(
        lora_path: str,
) -> list[tuple[str, SamplingParams, LoRARequest | None, str]]:
    """
    –°–æ–∑–¥–∞—ë–º –ø–∞—Ä—ã –ø—Ä–æ–º–ø—Ç–æ–≤: –æ–¥–∏–Ω –±–µ–∑ LoRA, –æ–¥–∏–Ω —Å LoRA
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (prompt, sampling_params, lora_request, label)
    """
    prompts = [
        "My name is",
        "The capital of USA is",
        "The capital of France is",
        "Explain what machine learning is:",
        "Write a short poem about the sea:",
    ]

    sampling_params = SamplingParams(temperature=0.0, logprobs=1, max_tokens=128)

    test_prompts = []
    for prompt in prompts:
        # –ë–µ–∑ LoRA (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å)
        test_prompts.append((
            prompt,
            sampling_params,
            None,
            "BASE"
        ))
        # –° LoRA (–¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å)
        test_prompts.append((
            prompt,
            sampling_params,
            LoRARequest("qlora-flan", 1, lora_path),
            "LORA"
        ))

    return test_prompts


def process_requests(
        engine: LLMEngine,
        test_prompts: list[tuple[str, SamplingParams, LoRARequest | None, str]],
):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –∏ —Å–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
    request_id = 0
    results = {}  # {request_id: (prompt, label, output)}
    id_to_info = {}  # {request_id: (prompt, label)}

    while test_prompts or engine.has_unfinished_requests():
        if test_prompts:
            prompt, sampling_params, lora_request, label = test_prompts.pop(0)
            engine.add_request(
                str(request_id), prompt, sampling_params, lora_request=lora_request
            )
            id_to_info[str(request_id)] = (prompt, label)
            request_id += 1

        request_outputs: list[RequestOutput] = engine.step()
        for request_output in request_outputs:
            if request_output.finished:
                req_id = request_output.request_id
                prompt, label = id_to_info[req_id]
                results[req_id] = (prompt, label, request_output.outputs[0].text)

    return results


def print_comparison(results: dict):
    """–í—ã–≤–æ–¥–∏–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ BASE vs LORA –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞."""

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–æ–º–ø—Ç–∞–º
    prompts_seen = []
    for req_id in sorted(results.keys(), key=int):
        prompt, label, output = results[req_id]
        if prompt not in prompts_seen:
            prompts_seen.append(prompt)

    for prompt in prompts_seen:
        print("=" * 70)
        print(f"PROMPT: {prompt}")
        print("=" * 70)

        # –ù–∞—Ö–æ–¥–∏–º BASE –∏ LORA —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        for req_id in sorted(results.keys(), key=int):
            p, label, output = results[req_id]
            if p == prompt:
                if label == "BASE":
                    print(f"\nüîµ BASE (–±–µ–∑ LoRA):")
                    print(f"   {output.strip()}")
                else:
                    print(f"\nüü¢ LORA (—Å LoRA):")
                    print(f"   {output.strip()}")

        print()


def initialize_engine(model: str, quantization: str) -> LLMEngine:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–≤–∏–∂–æ–∫ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π LoRA."""

    engine_args = EngineArgs(
        model=model,
        quantization=quantization,
        enable_lora=True,
        max_lora_rank=64,
        max_loras=4,
    )
    return LLMEngine.from_engine_args(engine_args)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""

    config = {
        "name": "qlora_inference_example",
        "model": "huggyllama/llama-7b",
        "quantization": "bitsandbytes",
        "lora_repo": "timdettmers/qlora-flan-7b",
    }

    print(f"–ú–æ–¥–µ–ª—å: {config['model']}")
    print(f"–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è: {config['quantization']}")
    print(f"LoRA –∞–¥–∞–ø—Ç–µ—Ä: {config['lora_repo']}")
    print()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–≤–∏–∂–æ–∫
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
    engine = initialize_engine(config["model"], config["quantization"])

    # –°–∫–∞—á–∏–≤–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä
    print("–°–∫–∞—á–∏–≤–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä...")
    lora_path = snapshot_download(repo_id=config["lora_repo"])

    # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    test_prompts = create_test_prompts(lora_path)

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã
    print("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã...")
    print()
    results = process_requests(engine, test_prompts)

    # –í—ã–≤–æ–¥–∏–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    print_comparison(results)

    # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
    del engine
    gc.collect()
    torch.cuda.empty_cache()


if __name__ == "__main__":
    main()